{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from collections import defaultdict\n",
    "import utm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import itertools\n",
    "\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.rdd import portable_hash\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from shapely.ops import nearest_points\n",
    "import geopy.distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyspark.sql.functions  import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"5g\").config(\"spark.executor.cores\", \"10\").config(\"spark.cores.max\", \"10\").config(\"spark.driver.host\", \"localhost\").config(\"spark.executor.memory\", \"5g\").config(\"spark.executor.core\",\n",
    "                             \"10\").appName(\"busgps\").getOrCreate()\n",
    "sc=SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "# sc = spark.SparkContext(\"local\", \"travelTime\")\n",
    "# sc = SparkContext(\"local\", \"First App\",conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_station = gpd.GeoDataFrame(columns=[\"station_id\", \"code\", \"number_in_line\",\n",
    "                                      \"line\", \"type\", \"station_Point\"])\n",
    "\n",
    "bus_gps_Record = namedtuple(\"bus_gps_Record\",\n",
    "                            [\"automative_id\", \"id\", \"sattime\", \"time\", \"speed\", \"latitude\", \"longitude\",\n",
    "                             \"station_latitude\", \"station_longitude\", 'station_code','type','number_in_line','station_distance'])\n",
    "\n",
    "station_Record = namedtuple(\"station_Record\", [\"station_id\", \"code\", \"number_in_line\",\n",
    "                                               \"line\", \"type\", \"station_Point\"])\n",
    "\n",
    "bus_travle_Record = namedtuple(\"bus_travel_Record\",\n",
    "                               [\"start_station_code\", \"end_station_code\", \"line\", \"time_part\", \"start_time\", \"day\",\n",
    "                                \"speed_variance\",\n",
    "                                \"speed_average\", \"start_station_distance\", \"end_station_distance\", \"travelTime\",\n",
    "                                \"end_time\" ,\"stations_distance\"])\n",
    "#stop_time_Record = namedtuple(\"stop_time_Record\", [\"station_id\", \"time\" ,\"stop_time\" ,\"station_distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for line in open(\"bus_station.txt\"):\n",
    "    fields = line.split(\"\\t\")\n",
    "    place = utm.to_latlon(float(fields[6]), float(fields[7]), 40, 'N')\n",
    "    raw_station.loc[i] = station_Record(int(fields[0]), int(fields[2]), fields[3], int(fields[4]), fields[5],\n",
    "                                       Point(place[0], place[1]))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.sparkContext.broadcast( raw_station)\n",
    "#type( raw_station.loc[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcute_distance(origin_latitude, origin_longitude, destination_latitude, destination_longitude):\n",
    "  \n",
    "  \n",
    "    coords_1 = (float(origin_latitude or 0.0), float(origin_longitude or 0.0))\n",
    "    coords_2 =  (float(destination_latitude or 0.0),float(destination_longitude or 0.0))\n",
    "\n",
    "    return (geopy.distance.geodesic(coords_1, coords_2).m)\n",
    "    # print(origin_latitude,origin_longitude,destination_latitude,destination_longitude,\"nnnnnnnnnnnnnnn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(y1,y2):\n",
    "    m = ((float)(y2-y1))\n",
    "    if m>0:   \n",
    "        return(True)\n",
    "    if m<0:\n",
    "        return(False)\n",
    "    if m==0:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBusStation(iterator):\n",
    "    busPath = []\n",
    "    bus = 0\n",
    "    result = []\n",
    "    for x in iterator:\n",
    "        bus = x.automative_id\n",
    "        number=x.number_in_line\n",
    "        if int(number) not in busPath:\n",
    "            busPath.append(int(number))\n",
    "    result.append(bus)\n",
    "    result.extend(busPath)\n",
    "    return result, '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCommonStation(x):\n",
    "    if x == '*':\n",
    "        return None\n",
    "\n",
    "    bus = x[0]\n",
    "    busStations = x[1:]\n",
    "\n",
    "    maxLen = 0\n",
    "    maxline = 0\n",
    "    commonpath = []\n",
    "    for line in lines_path.value:\n",
    "        stations = lines_path.value[line]\n",
    "        common = list(set(stations).intersection(busStations))\n",
    "        if len(common) > maxLen:\n",
    "            maxLen = len(common)\n",
    "            maxline = line\n",
    "            commonpath = common\n",
    "    return bus, maxline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bus_gps(s):\n",
    "    \n",
    "    try:\n",
    "  \n",
    "        fields = s.split(',')\n",
    "\n",
    "        time = datetime.utcfromtimestamp(float(fields[6]))+timedelta(hours=3,minutes=30)\n",
    "        number_bus = int(fields[1])\n",
    "        \n",
    "        \n",
    "        #\n",
    "\n",
    "\n",
    "       # line_b = bus_Line.value[int(fields[1])]\n",
    "\n",
    "\n",
    "        #number_bus = 7 \n",
    "\n",
    "        #if int(fields[1] )!=number_bus: \n",
    "         #   return None\n",
    "\n",
    "        #if int(line_b[0])  not in  [50]:\n",
    "         #   return None\n",
    "\n",
    "       # if float(fields[6]) > 1476824529:\n",
    "         #   return None\n",
    "\n",
    "        #if time.hour > 21 or time.hour < 6:\n",
    "       #     return None\n",
    "\n",
    "        longitude_bus = float(fields[13].split(' ')[2].replace('(', ''))\n",
    "        latitude_bus = float(fields[13].split(' ')[3])\n",
    "\n",
    "        # for index, row in stations.value.iterrows():\n",
    "        #     distances[i]=distance(latitude,longitude,row['station_latitude'],raw_station['station_longitude'])\n",
    "        # station_distance=0\n",
    "        station_distance = float(\"inf\")\n",
    "\n",
    "        # station_distance_list=[]\n",
    "        # station_code_list=[]\n",
    "        # station_code=-1\n",
    "        number_in_line = -1\n",
    "        station_code = -1\n",
    "        station_type = -1\n",
    "        # number_in_line_list=[]\n",
    "        line = -1\n",
    "        # line_list=[]\n",
    "\n",
    "        orig = Point(latitude_bus, longitude_bus)\n",
    "\n",
    "        \n",
    "        lineBus =bus_Line.value[int(number_bus)]\n",
    "        lineStations =MultiPoint(lines_point_went.value[lineBus[0]])\n",
    "        nearest_geoms = nearest_points(orig, lineStations)\n",
    "        near_idx0 = nearest_geoms[1]\n",
    "\n",
    "        \n",
    "        station = stations.value.query(\"line== @lineBus[0]\")\n",
    "        filter3 = station[\"station_Point\"] == Point(near_idx0.x ,near_idx0.y)\n",
    "        near = station[filter3]\n",
    "\n",
    "        distance = calcute_distance(latitude_bus, longitude_bus, near_idx0.x, near_idx0.y)\n",
    "        finaldistance=min_distance(distance)\n",
    "        \n",
    "        return bus_gps_Record( int(fields[1]), fields[0], float(fields[6]), time.strftime('%Y-%m-%d-%H:%M:%S-%p'),\n",
    "                               float(fields[7])\n",
    "                               , latitude_bus, longitude_bus, near_idx0.x, near_idx0.y, near['code'].values[0],near['type'].values[0],int(near['number_in_line'].values[0]), finaldistance)\n",
    "\n",
    "\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_distance(x):\n",
    "    if x<100:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitioner(n):\n",
    "    \"\"\"Partition by the first item in the key tuple\"\"\"\n",
    "\n",
    "    def partitioner_(x):\n",
    "        return portable_hash(x[0]) % n\n",
    "\n",
    "    return partitioner_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_record(r):\n",
    "    for i in range(0,len(r)):\n",
    "        x = (list(r[1]))\n",
    "        return bus_gps_Record(r[0], x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9],x[10],x[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_dict = {}\n",
    "for line in open(\"sorted_line.csv\"):\n",
    "    fields = line.split(\",\")\n",
    "    if int(fields[4]) in line_dict:\n",
    "        # append the new number to the existing array at this slot\n",
    "        line_dict[int(fields[4])].append(int(fields[2]))\n",
    "    else:\n",
    "        # create a new array in this slot\n",
    "        line_dict[int(fields[4])] = [int(fields[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_path = spark.sparkContext.broadcast(line_dict)\n",
    "#lines_path.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_dict = {}\n",
    "for line in open(\"bus_line.txt\"):\n",
    "    fields = line.replace(\"(\", \"\").replace(\")\", \"\").split(\",\")\n",
    "    if int(fields[0]) == 0:\n",
    "        continue\n",
    "    if int(fields[0]) in bus_dict:\n",
    "        # append the new number to the existing array at this slot\n",
    "        bus_dict[int(fields[0])].append(int(fields[1]))\n",
    "    else:\n",
    "        # create a new array in this slot\n",
    "        bus_dict[int(fields[0])] = [int(fields[1])]\n",
    "\n",
    "# raw_station.to_csv(\"stations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_Line = spark.sparkContext.broadcast(bus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_went_points_dict = {}\n",
    "for line in open(\"sorted_line.csv\"):\n",
    "    fields = line.split(\",\")\n",
    "    status = fields[5]\n",
    "    if status == 'R':\n",
    "        place = utm.to_latlon(float(fields[6]), float(fields[7]), 40, 'N')\n",
    "\n",
    "        if int(fields[4]) in stations_went_points_dict:\n",
    "            # append the new number to the existing array at this slot\n",
    "            points = []\n",
    "            stations_went_points_dict[int(fields[4])].append(Point(place[0], place[1]))\n",
    "        else:\n",
    "            # create a new array in this slot\n",
    "\n",
    "            stations_went_points_dict[int(fields[4])] = [Point(place[0], place[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_point_went =spark.sparkContext.broadcast(stations_went_points_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_return_points_dict = {}\n",
    "for line in open(\"sorted_line.csv\"):\n",
    "    fields = line.split(\",\")\n",
    "    status = fields[5]\n",
    "    if status == 'B':\n",
    "        place = utm.to_latlon(float(fields[6]), float(fields[7]), 40, 'N')\n",
    "\n",
    "        if int(fields[4]) in stations_return_points_dict:\n",
    "            # append the new number to the existing array at this slot\n",
    "            points = []\n",
    "            stations_return_points_dict[int(fields[4])].append(Point(place[0], place[1]))\n",
    "        else:\n",
    "            # create a new array in this slot\n",
    "\n",
    "            stations_return_points_dict[int(fields[4])] = [Point(place[0], place[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_point_return =spark.sparkContext.broadcast(stations_return_points_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n",
      "4739\n",
      "0kkkkkkkkkkkkkkkkkkkkkkkkk 217\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'withColumn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-70bcabfd0efd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#final.saveAsTextFile(\"/Users/neginkzm/Desktop/testgps/test1/busfinal4\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"partitionId\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspark_partition_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"partitionId\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#tra= final.mapPartitions(trajectory)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'withColumn'"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "\n",
    "#raw_bus = spark.sparkContext.textFile(\"sample.csv\")\n",
    "raw_bus = spark.sparkContext.textFile(\"avlenddrop_geodata_1476649800_1477254599.csv\")\n",
    "print(raw_bus.count())\n",
    "\n",
    "parsed_bus_gps = raw_bus.map(parse_bus_gps).filter(lambda x: (x != None))\n",
    "parsed_bus= parsed_bus_gps.filter(lambda x: (x[12] != None))\n",
    "\n",
    "\n",
    "print(parsed_bus.count())\n",
    "#parsed_bus.repartition(1).saveAsTextFile(\"/Users/neginkzm/Desktop/busgps/busid_7/parse1\")\n",
    "\n",
    "##maxNumber of automative_id\n",
    "parsed_bus.cache()\n",
    "num_bus = parsed_bus.map(lambda x: x[0]).max()\n",
    "print('0kkkkkkkkkkkkkkkkkkkkkkkkk', num_bus)\n",
    "\n",
    "#a = spark.sparkContext.parallelize(range(int(1e6)), num_bus)\n",
    "#l = a.glom().map(len).collect()  # get length of each partition\n",
    "#print(min(l), max(l), sum(l)/len(l), len(l))  # check if skewed\n",
    "\n",
    "#پارتیشن بندی بر اساس شماره اتوبوس\n",
    "mapped_by_key = parsed_bus.map(lambda x: (x.automative_id, x[1:]))\n",
    "#mapped_by_key.saveAsTextFile(\"/Users/neginkzm/Desktop/busgps/busid_7/mapped2\")\n",
    "\n",
    "\n",
    "partitionBy_automative_id = mapped_by_key.keyBy(lambda kv: (kv[0], kv[1][1])).repartitionAndSortWithinPartitions(\n",
    "    numPartitions=num_bus, partitionFunc=partitioner(num_bus), ascending=True).map(lambda x: x[1])\n",
    "\n",
    "#partitionBy_automative_id.saveAsTextFile(\"/Users/neginkzm/Desktop/testgps/test1/buspartition3\")\n",
    "\n",
    "\n",
    "final = partitionBy_automative_id.map(map_to_record)\n",
    "#final.saveAsTextFile(\"/Users/neginkzm/Desktop/testgps/test1/busfinal4\")\n",
    "\n",
    "\n",
    "#tra= final.mapPartitions(trajectory)\n",
    "#tra.saveAsTextFile(\"/Users/neginkzm/Desktop/testgps/tra1/trajectory\")\n",
    "\n",
    "\n",
    "# find line of bus\n",
    "##busStations = final.mapPartitions(findBusStation).repartition(1)\n",
    "##busStations.saveAsTextFile(\"/Users/neginkzm/Desktop/busgps/busid_7/findbusstation5\")\n",
    "\n",
    "##busLine = busStations.map(findCommonStation)\n",
    "##busLine.saveAsTextFile(\"/Users/neginkzm/Desktop/busgps/busid_7/busline6\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
