{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from scipy import spatial\n",
    "from NPIR import NPIR\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest,desc,asc, row_number, avg, mean, least, struct, lit, sequence\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row\n",
    "from pyspark import SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "# config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "# config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"5g\").\\\n",
    "# config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "# config(\"spark.memory.offHeap.size\", \"16g\").appName(\"mykmeans\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = spark.sparkContext\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #read csv\n",
    "# data_spark_df = spark.read.format('csv').option('header','False').option('index','False').load('seeds.csv')\n",
    "\n",
    "# data_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_spark_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark_df = data_spark_df.select('_c3', '_c4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|first|second|\n",
      "+-----+------+\n",
      "|5.763| 3.312|\n",
      "|5.554| 3.333|\n",
      "|5.291| 3.337|\n",
      "|5.324| 3.379|\n",
      "|5.658| 3.562|\n",
      "|5.386| 3.312|\n",
      "|5.563| 3.259|\n",
      "| 5.42| 3.302|\n",
      "|6.053| 3.465|\n",
      "|5.884| 3.505|\n",
      "|5.714| 3.242|\n",
      "|5.438| 3.201|\n",
      "|5.439| 3.199|\n",
      "|5.479| 3.156|\n",
      "|5.482| 3.114|\n",
      "|5.351| 3.333|\n",
      "|5.119| 3.383|\n",
      "|5.527| 3.514|\n",
      "|5.205| 3.466|\n",
      "|5.226| 3.049|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new_name = ['first', 'second']\n",
    "# data_spark_df = data_spark_df.toDF(*new_name)\n",
    "\n",
    "# data_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "\n",
    "# spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "\n",
    "# spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "# sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first: string, second: string, first_numeric: float]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_spark_df = data_spark_df.withColumn(\"first_numeric\", data_spark_df[\"first\"].cast(FloatType()))\n",
    "# data_spark_df = data_spark_df.withColumn(\"second_numeric\", data_spark_df[\"second\"].cast(FloatType()))\n",
    "# data_spark_df = data_spark_df.drop('first').drop('second')\n",
    "# data_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:00.015702\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('blobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "label = NPIR(data.values[:,:-1],k,IR,i)\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxIndex = IR * data_spark_df.count()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark_df.foreachPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(data_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34579087654324568"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (34579087654324567890//1000)+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
