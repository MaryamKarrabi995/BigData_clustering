{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from scipy import spatial\n",
    "from NPIR import NPIR\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from collections import Counter as Cs\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import *\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType,\\\n",
    "StructField,ArrayType\n",
    "from pyspark.sql.functions import udf, log, rand, monotonically_increasing_id, col, broadcast,\\\n",
    "greatest,desc,asc, row_number, avg, mean, least, struct, lit, sequence\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row\n",
    "from pyspark import SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.broadcastTimeout\", \"30000s\").\\\n",
    "# config(\"spark.network.timeout\",\"30000s\").config(\"spark.executor.heartbeatInterval\",\"12000000ms\").\\\n",
    "# config(\"spark.storage.blockManagerSlaveTimeoutMs\",\"12000001ms\").config(\"spark.driver.maxResultSize\",\"5g\").\\\n",
    "# config(\"spark.default.parallelism\", \"100\").config(\"spark.memory.offHeap.enabled\",\"true\").\\\n",
    "# config(\"spark.memory.offHeap.size\", \"16g\").appName(\"mykmeans\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = spark.sparkContext\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #read csv\n",
    "# data_spark_df = spark.read.format('csv').option('header','False').option('index','False').load('seeds.csv')\n",
    "\n",
    "# data_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_spark_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark_df = data_spark_df.select('_c3', '_c4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|first|second|\n",
      "+-----+------+\n",
      "|5.763| 3.312|\n",
      "|5.554| 3.333|\n",
      "|5.291| 3.337|\n",
      "|5.324| 3.379|\n",
      "|5.658| 3.562|\n",
      "|5.386| 3.312|\n",
      "|5.563| 3.259|\n",
      "| 5.42| 3.302|\n",
      "|6.053| 3.465|\n",
      "|5.884| 3.505|\n",
      "|5.714| 3.242|\n",
      "|5.438| 3.201|\n",
      "|5.439| 3.199|\n",
      "|5.479| 3.156|\n",
      "|5.482| 3.114|\n",
      "|5.351| 3.333|\n",
      "|5.119| 3.383|\n",
      "|5.527| 3.514|\n",
      "|5.205| 3.466|\n",
      "|5.226| 3.049|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new_name = ['first', 'second']\n",
    "# data_spark_df = data_spark_df.toDF(*new_name)\n",
    "\n",
    "# data_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "\n",
    "# spark.conf.set('spark.jars.packages','com.databricks:spark-cav_2.11')\n",
    "\n",
    "# spark.conf.set(\"spark.sql.parquet.compression.codec\",\"gzip\")\n",
    "\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "# sqlContext.sql(\"SET spark.sql.autoBroadcastJoinThreshold = -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first: string, second: string, first_numeric: float]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_spark_df = data_spark_df.withColumn(\"first_numeric\", data_spark_df[\"first\"].cast(FloatType()))\n",
    "# data_spark_df = data_spark_df.withColumn(\"second_numeric\", data_spark_df[\"second\"].cast(FloatType()))\n",
    "# data_spark_df = data_spark_df.drop('first').drop('second')\n",
    "# data_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IR: The indexing ratio to be used for generating the maximum index\n",
    "IR = 0.2\n",
    "#The number of iteration i\n",
    "i = 10\n",
    "k = 3 #k: Number of clusters\n",
    "# count = Cs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:00.015702\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('blobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d37ba30bb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNPIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Execution time HH:MM:SS:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amin/Github/BigData_clustering/NPIR.py\u001b[0m in \u001b[0;36mNPIR\u001b[0;34m(points_, k, IR, i)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabelsPred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amin/Github/BigData_clustering/NPIR.py\u001b[0m in \u001b[0;36mcalculate\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0;31m#check if the Nearest should move to the cluster of the Elected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mshouldPointMoveToNearerCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnearestIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnearestDist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Case 3b: The distance between the Nearest and the Assigner is greater than the distance between the Nearest and the Elected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                             \u001b[0mmoveNearestToElectedCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melectedIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnearestIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnearestDist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The Nearest moves to the cluster of the elected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                         \u001b[0mchangeAssigner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnearestIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melectedIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnearestDist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amin/Github/BigData_clustering/NPIR.py\u001b[0m in \u001b[0;36mmoveNearestToElectedCluster\u001b[0;34m(electedIndex, nearestIndex, distance)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;31m# Change the cluster of the Nearest to the cluster of the Elected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0maddPointToCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusterNo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnearestIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melectedIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;31m# This to ensure that no cluster is left empty. K is preserved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;31m# Add a new point to the empty cluster from the remaining points. if no remaining points still exist, we add the point from the already assigned points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amin/Github/BigData_clustering/NPIR.py\u001b[0m in \u001b[0;36maddPointToCluster\u001b[0;34m(clusterNo, pointIndex, assignerIndex, assignerDistance)\u001b[0m\n\u001b[1;32m    437\u001b[0m         '''\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0mupdateAssignmentsTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0massignerIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0;31m#testTree()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amin/Github/BigData_clustering/NPIR.py\u001b[0m in \u001b[0;36mupdateAssignmentsTree\u001b[0;34m(pointIndex, assignerIndex)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0massignmentsTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Point already assigned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mpointTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massignmentsTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0massignmentsTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpointIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0massignmentsTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignerIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amin/.local/lib/python3.6/site-packages/treelib/tree.py\u001b[0m in \u001b[0;36mremove_node\u001b[0;34m(self, identifier)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;31m# Remove node and its children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mremoved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremoved\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amin/.local/lib/python3.6/site-packages/treelib/tree.py\u001b[0m in \u001b[0;36mexpand_tree\u001b[0;34m(self, nid, mode, filter, key, reverse, sorting)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m                     expansion = [self[i] for i in queue[0].successors(self._identifier)\n\u001b[1;32m    439\u001b[0m                                  if filter(self[i])]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "label = NPIR(data.values[:,:-1],k,IR,i)\n",
    "\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxIndex = IR * data_spark_df.count()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_spark_df.foreachPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(data_spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34579087654324568"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (34579087654324567890//1000)+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
