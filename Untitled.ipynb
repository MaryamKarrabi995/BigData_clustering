{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_dist_explore import hist\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Window, Row\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, StringType, StructType, StructField,ArrayType\n",
    "from pyspark.sql.functions import (udf, log, rand, monotonically_increasing_id, col, broadcast,greatest,desc,asc,\n",
    "row_number, avg, mean, least, struct, lit, sequence)\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"First App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 24, lines with b: 8\n"
     ]
    }
   ],
   "source": [
    "logFile = \"/home/amin/Github/BigData_clustering/Untitled.ipynb\"  \n",
    "\n",
    "logData = sc.textFile(logFile).cache()\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[37] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD -> 8\n"
     ]
    }
   ],
   "source": [
    "counts = words.count()\n",
    "print(\"Number of elements in RDD -> %i\" % (counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "coll = words.collect()\n",
    "print(\"Elements in RDD -> %s\" % (coll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    print(x)\n",
    "fore = words.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Fitered RDD -> %s\" % (filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair -> [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(\"Key value pair -> %s\" % (mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 3\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)/nums.count()\n",
    "print(\"Adding all the elements -> %i\" % (adding))\n",
    "\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(type(nums))\n",
    "adding = nums.filter(lambda x: x!=2)\n",
    "print(type(adding))\n",
    "adding.reduce(add)\n",
    "# print(\"Adding all the elements -> %i\" % (adding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonprogrammingisawesome!\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "x = ['Python', 'programming', 'is', 'awesome!']\n",
    "print(reduce(lambda val1, val2: val1 + val2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print(\"Join RDD -> %s\" % (final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
